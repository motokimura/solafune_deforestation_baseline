{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a part of motokimura's baseline solution for the [Solafune Identifying Deforestation Drivers competition](https://solafune.com/competitions/68ad4759-4686-4bb3-94b8-7063f755b43d?menu=about&tab=overview).\n",
    "See https://github.com/motokimura/solafune_deforestation_baseline for the complete code.\n",
    "\n",
    "> cf. @solafune (https://solafune.com) Use for any purpose other than participation in the competition or commercial use is prohibited. If you would like to use them for any of the above purposes, please contact us.\n",
    "\n",
    "### Description\n",
    "\n",
    "**By running this notebook, you will achieve a score of around 0.227 on the public leaderboard.**\n",
    "\n",
    "This notebook trains a U-Net model for 4-class segmentation (`grassland_shrubland`, `logging`, `mining`, and `plantation`) and generates a submission JSON file for the evaluation images from the output from the U-Net model.\n",
    "\n",
    "The submission JSON file is saved to `data/submission.json`.\n",
    "\n",
    "Before running this notebook, you have to run `generate_masks.ipynb` to generate `.npy` files used for training\n",
    "(`generate_masks.ipynb` is available from https://github.com/motokimura/solafune_deforestation_baseline).\n",
    "\n",
    "### Requirements\n",
    "\n",
    "#### Datasets\n",
    "\n",
    "Organize the dataset as follows:\n",
    "\n",
    "```\n",
    "data/\n",
    "├── evaluation_images/\n",
    "│   ├── evaluation_0.tif\n",
    "│   ├── evaluation_1.tif\n",
    "│   ├── evaluation_2.tif\n",
    "│   ├── ...\n",
    "├── train_images/\n",
    "│   ├── train_0.tif\n",
    "│   ├── train_1.tif\n",
    "│   ├── train_2.tif\n",
    "│   ├── ...\n",
    "├── train_masks/\n",
    "│   ├── train_0.npy\n",
    "│   ├── train_1.npy\n",
    "│   ├── train_2.npy\n",
    "│   ├── ...\n",
    "```\n",
    "\n",
    "`evaluation_images` and `train_images` can be downloaded from the competition page.\n",
    "\n",
    "`train_masks` can be generated by running `generate_masks.ipynb` available from https://github.com/motokimura/solafune_deforestation_baseline.\n",
    "\n",
    "#### Libraries\n",
    "\n",
    "Please install the python packages imported the cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import albumentations as albu  # tested with 1.4.24\n",
    "import imagecodecs\n",
    "import numpy as np  # tested with 1.26.4\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl  # tested with 2.5.0.post0\n",
    "import segmentation_models_pytorch as smp  # tested with 0.3.4\n",
    "import sklearn\n",
    "import tensorboard\n",
    "import tifffile\n",
    "import timm  # tested with 0.9.7\n",
    "import torch  # tested with 2.5.1\n",
    "\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from rasterio import features\n",
    "from shapely.geometry import Polygon, shape\n",
    "from skimage import measure\n",
    "from solafune_tools.metrics import F1_Metrics\n",
    "from timm.optim import create_optimizer_v2\n",
    "from timm.scheduler import create_scheduler_v2\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = Path(\"./data\")\n",
    "\n",
    "class_names = [\"grassland_shrubland\", \"logging\", \"mining\", \"plantation\"]\n",
    "\n",
    "epochs = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define dataset class to load images and masks for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mask(mask_path):\n",
    "    mask = np.load(mask_path)  # (4, H, W), uint8\n",
    "    assert mask.shape == (4, 1024, 1024)\n",
    "    mask = mask.transpose(1, 2, 0)  # (H, W, 4)\n",
    "    return mask.astype(np.float32) / 255.0  # normalize to [0, 1]\n",
    "\n",
    "\n",
    "def load_image(image_path):\n",
    "    image = tifffile.imread(image_path)  # (H, W, 12), float64\n",
    "    assert image.shape == (1024, 1024, 12)\n",
    "    image = np.nan_to_num(image)  # replace NaN with 0\n",
    "    return image.astype(np.float32)\n",
    "\n",
    "\n",
    "def normalize_image(image):\n",
    "    # mean of train images\n",
    "    mean = np.array(\n",
    "        [\n",
    "            285.8190561180765,\n",
    "            327.22091430696577,\n",
    "            552.9305957826701,\n",
    "            392.1575148484924,\n",
    "            914.3138803812591,\n",
    "            2346.1184507500043,\n",
    "            2884.4831706095824,\n",
    "            2886.442429854111,\n",
    "            3176.7501338557763,\n",
    "            3156.934442092072,\n",
    "            1727.1940075511282,\n",
    "            848.573373995044,\n",
    "        ],\n",
    "        dtype=np.float32\n",
    "    )\n",
    "\n",
    "    # std of train images\n",
    "    std = np.array(\n",
    "        [\n",
    "            216.44975668759372,\n",
    "            269.8880248304874,\n",
    "            309.92790753407064,\n",
    "            397.45655590699,\n",
    "            400.22078920482215,\n",
    "            630.3269651264278,\n",
    "            789.8006920468097,\n",
    "            810.4773696969773,\n",
    "            852.9031432100967,\n",
    "            807.5976198303886,\n",
    "            631.7808113929271,\n",
    "            502.66788721341396,\n",
    "        ],\n",
    "        dtype=np.float32\n",
    "    )\n",
    "    \n",
    "    mean = mean.reshape(12, 1, 1)\n",
    "    std = std.reshape(12, 1, 1)\n",
    "\n",
    "    return (image - mean) / std\n",
    "\n",
    "\n",
    "class TrainValDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_root, sample_indices, augmentations=None):\n",
    "        self.image_paths, self.mask_paths = [], []\n",
    "        for i in sample_indices:\n",
    "            self.image_paths.append(data_root / \"train_images\" / f\"train_{i}.tif\")\n",
    "            self.mask_paths.append(data_root / \"train_masks\" / f\"train_{i}.npy\")\n",
    "        self.augmentations = augmentations\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {\n",
    "            \"image\": load_image(self.image_paths[idx]),\n",
    "            \"mask\": load_mask(self.mask_paths[idx]),\n",
    "        }\n",
    "\n",
    "        if self.augmentations is not None:\n",
    "            sample = self.augmentations(**sample)\n",
    "\n",
    "        sample[\"image\"] = sample[\"image\"].transpose(2, 0, 1)  # (12, H, W)\n",
    "        sample[\"mask\"] = sample[\"mask\"].transpose(2, 0, 1)  # (4, H, W)\n",
    "\n",
    "        sample[\"image\"] = normalize_image(sample[\"image\"])\n",
    "\n",
    "        # add metadata\n",
    "        sample[\"image_path\"] = str(self.image_paths[idx])\n",
    "        sample[\"mask_path\"] = str(self.mask_paths[idx])\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define U-Net model using pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # prepare segmentation model\n",
    "        self.model = m = smp.create_model(\n",
    "            arch='unet',\n",
    "            encoder_name='tu-tf_efficientnetv2_s',  # use `tf_efficientnetv2_s` from timm\n",
    "            encoder_weights='imagenet',  # always starts from imagenet pre-trained weight\n",
    "            in_channels=12,\n",
    "            classes=4,\n",
    "        )\n",
    "\n",
    "        # prepare loss functions\n",
    "        self.dice_loss_fn = smp.losses.DiceLoss(mode=smp.losses.MULTILABEL_MODE, from_logits=True)\n",
    "        self.bce_loss_fn = smp.losses.SoftBCEWithLogitsLoss(smooth_factor=0.0)\n",
    "\n",
    "        self.training_step_outputs = []\n",
    "        self.validation_step_outputs = []\n",
    "    \n",
    "    def forward(self, image):\n",
    "        # assuming image is already normalized\n",
    "        return self.model(image)  # logits\n",
    "\n",
    "    def shared_step(self, batch, stage):\n",
    "        image = batch[\"image\"]\n",
    "        mask = batch[\"mask\"]\n",
    "\n",
    "        logits_mask = self.forward(image)\n",
    "\n",
    "        loss = self.dice_loss_fn(logits_mask, mask) + self.bce_loss_fn(logits_mask, mask)\n",
    "\n",
    "        # count tp, fp, fn, tn for each class to compute validation metrics at the end of epoch\n",
    "        thresh = 0.5\n",
    "        prob_mask = logits_mask.sigmoid()\n",
    "        tp, fp, fn, tn = smp.metrics.get_stats(\n",
    "            (prob_mask > thresh).long(),\n",
    "            mask.long(),\n",
    "            mode=smp.losses.MULTILABEL_MODE,\n",
    "        )  # each of tp, fp, fn, tn is a tensor of shape (batch_size, num_classes) and of type long\n",
    "\n",
    "        output = {\n",
    "            \"loss\": loss.detach().cpu(),\n",
    "            \"tp\": tp.detach().cpu(),\n",
    "            \"fp\": fp.detach().cpu(),\n",
    "            \"fn\": fn.detach().cpu(),\n",
    "            \"tn\": tn.detach().cpu(),\n",
    "        }\n",
    "        if stage == \"train\":\n",
    "            self.training_step_outputs.append(output)\n",
    "        else:\n",
    "            self.validation_step_outputs.append(output)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.shared_step(batch, \"train\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.shared_step(batch, \"val\")\n",
    "\n",
    "    def shared_epoch_end(self, outputs, stage):\n",
    "        def log(name, tensor, prog_bar=False):\n",
    "            self.log(f\"{stage}/{name}\", tensor.to(self.device), sync_dist=True, prog_bar=prog_bar)\n",
    "\n",
    "        # aggregate loss\n",
    "        loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
    "        log(\"loss\", loss, prog_bar=True)\n",
    "\n",
    "        # aggregate tp, fp, fn, tn to compose IoU score for each class\n",
    "        tp = torch.cat([x[\"tp\"] for x in outputs])\n",
    "        fp = torch.cat([x[\"fp\"] for x in outputs])\n",
    "        fn = torch.cat([x[\"fn\"] for x in outputs])\n",
    "        tn = torch.cat([x[\"tn\"] for x in outputs])\n",
    "\n",
    "        iou_scores = {}\n",
    "        for i, class_name in enumerate(class_names):\n",
    "            iou_scores[class_name] = smp.metrics.iou_score(tp[:, i], fp[:, i], fn[:, i], tn[:, i], reduction=\"macro-imagewise\")\n",
    "            log(f\"iou/{class_name}\", iou_scores[class_name], prog_bar=False)\n",
    "\n",
    "        iou_avg = torch.stack([v for v in iou_scores.values()]).mean()\n",
    "        log(\"iou\", iou_avg, prog_bar=True)\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        self.shared_epoch_end(self.training_step_outputs, \"train\")\n",
    "        self.training_step_outputs.clear()\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        self.shared_epoch_end(self.validation_step_outputs, \"val\")\n",
    "        self.validation_step_outputs.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # optimizer\n",
    "        optimizer = create_optimizer_v2(\n",
    "            self.parameters(),\n",
    "            opt=\"adamw\",\n",
    "            lr=1e-4,\n",
    "            weight_decay=1e-2,\n",
    "            filter_bias_and_bn=True,  # filter out bias and batchnorm from weight decay\n",
    "        )\n",
    "\n",
    "        # lr scheduler\n",
    "        scheduler, _ = create_scheduler_v2(\n",
    "            optimizer,\n",
    "            sched=\"cosine\",\n",
    "            num_epochs=epochs,\n",
    "            min_lr=0.0,\n",
    "            warmup_lr=1e-5,\n",
    "            warmup_epochs=0,\n",
    "            warmup_prefix=False,\n",
    "            step_on_epochs=True,\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"interval\": \"epoch\",\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def lr_scheduler_step(self, scheduler, metric):\n",
    "        # workaround for timm's scheduler:\n",
    "        # https://github.com/Lightning-AI/lightning/issues/5555#issuecomment-1065894281\n",
    "        scheduler.step(epoch=self.current_epoch)  # timm's scheduler need the epoch value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare trainer of pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/tf_efficientnetv2_s.in21k_ft_in1k)\n",
      "INFO:timm.models._hub:[timm/tf_efficientnetv2_s.in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "INFO:timm.models._builder:Converted input conv conv_stem pretrained weights from 3 to 12 channel(s)\n"
     ]
    }
   ],
   "source": [
    "train_output_dir = data_root / \"training_result\"\n",
    "\n",
    "# split train_images into train-set and val-set\n",
    "sample_indices = list(range(176))  # train_0.tif to train_175.tif\n",
    "train_indices, val_indices = sklearn.model_selection.train_test_split(sample_indices, test_size=0.2, random_state=42)\n",
    "\n",
    "# augmentations applied only to train-set\n",
    "augmentations = albu.Compose(\n",
    "    [\n",
    "        # shift, scale, and rotate\n",
    "        albu.ShiftScaleRotate(\n",
    "            p=0.5,\n",
    "            shift_limit=0.0625,\n",
    "            scale_limit=0.1,\n",
    "            rotate_limit=15,\n",
    "            border_mode = 0,  # constant border\n",
    "            value = 0,\n",
    "            mask_value = 0,\n",
    "            interpolation = 2,  # bicubic\n",
    "        ),\n",
    "        # random crop\n",
    "        albu.RandomCrop(\n",
    "            p=1,\n",
    "            width=512,\n",
    "            height=512,\n",
    "        ),\n",
    "        # flip, transpose, and rotate90\n",
    "        albu.HorizontalFlip(p=0.5),\n",
    "        albu.VerticalFlip(p=0.5),\n",
    "        albu.Transpose(p=0.5),\n",
    "        albu.RandomRotate90(p=0.5),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    TrainValDataset(\n",
    "        data_root,\n",
    "        train_indices,\n",
    "        augmentations=augmentations,\n",
    "    ),\n",
    "    batch_size=16,\n",
    "    num_workers=8,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    TrainValDataset(\n",
    "        data_root,\n",
    "        val_indices,\n",
    "        augmentations=None,\n",
    "    ),\n",
    "    batch_size=4,\n",
    "    num_workers=8,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "# prepare trainer\n",
    "trainer = Trainer(\n",
    "    max_epochs=epochs,\n",
    "    callbacks = [\n",
    "        # save model with best validation IoU score\n",
    "        ModelCheckpoint(\n",
    "            dirpath=train_output_dir,\n",
    "            filename=\"best_iou_05\",\n",
    "            save_weights_only=True,\n",
    "            save_top_k=1,\n",
    "            monitor=\"val/iou\",\n",
    "            mode=\"max\",\n",
    "            save_last=False,\n",
    "        ),\n",
    "        LearningRateMonitor(logging_interval=\"step\"),\n",
    "    ],\n",
    "    logger=[TensorBoardLogger(train_output_dir, name=None)],\n",
    "    precision=\"16-mixed\",\n",
    "    deterministic=True,\n",
    "    benchmark=False,\n",
    "    sync_batchnorm=False,\n",
    "    check_val_every_n_epoch=5,\n",
    "    default_root_dir=os.getcwd(),\n",
    "    accelerator=\"gpu\",\n",
    "    devices=[0,],\n",
    "    strategy=\"ddp_notebook\",\n",
    "    log_every_n_steps=5,\n",
    ")\n",
    "\n",
    "# prepare model\n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training!\n",
    "\n",
    "With the default setting, 10 GB of GPU memory is required. To reduce the memory usage, you can decrease the batch size.\n",
    "\n",
    "The trained model is saved as `data/training_result/best_iou_05.ckpt`.\n",
    "\n",
    "Tensorboard logs are also saved under `data/training_result/version_xx`.\n",
    "\n",
    "**The execution often does not finish even after reaching 200 epochs. In that case, you can stop the execution manually and just proceed to the next cell (do not restart the notebook!).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /workspace/data/training_result exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name         | Type                  | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | model        | Unet                  | 22.1 M | train\n",
      "1 | dice_loss_fn | DiceLoss              | 0      | train\n",
      "2 | bce_loss_fn  | SoftBCEWithLogitsLoss | 0      | train\n",
      "---------------------------------------------------------------\n",
      "22.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "22.1 M    Total params\n",
      "88.383    Total estimated model params size (MB)\n",
      "759       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 199: 100%|██████████| 9/9 [00:13<00:00,  0.65it/s, v_num=0, train/loss=0.402, train/iou=0.781, val/loss=0.714, val/iou=0.534]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 199: 100%|██████████| 9/9 [00:13<00:00,  0.64it/s, v_num=0, train/loss=0.402, train/iou=0.781, val/loss=0.714, val/iou=0.534]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:46\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlauncher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlaunch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pytorch_lightning/strategies/launchers/multiprocessing.py:144\u001b[0m, in \u001b[0;36m_MultiProcessingLauncher.launch\u001b[0;34m(self, function, trainer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocs \u001b[38;5;241m=\u001b[39m process_context\u001b[38;5;241m.\u001b[39mprocesses\n\u001b[0;32m--> 144\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mprocess_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/multiprocessing/spawn.py:132\u001b[0m, in \u001b[0;36mProcessContext.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m# Wait for any process to fail or all of them to succeed.\u001b[39;00m\n\u001b[0;32m--> 132\u001b[0m ready \u001b[38;5;241m=\u001b[39m \u001b[43mmultiprocessing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentinels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m error_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/multiprocessing/connection.py:948\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 948\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    949\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# start training\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:539\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 539\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:64\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(launcher, _SubprocessScriptLauncher):\n\u001b[1;32m     63\u001b[0m         launcher\u001b[38;5;241m.\u001b[39mkill(_get_sigkill_signal())\n\u001b[0;32m---> 64\u001b[0m     \u001b[43mexit\u001b[49m(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[1;32m     67\u001b[0m     _interrupt(trainer, exception)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "trainer.fit(\n",
    "    model,\n",
    "    train_dataloaders=train_loader,\n",
    "    val_dataloaders=val_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the evaluation metric for the validation set\n",
    "\n",
    "Before predicting the evaluation images, let's predict the validation set and compute the evaluation metric.\n",
    "\n",
    "This may be useful to check if the model is trained properly and to tune the parameters for post-processing (e.g., score threshold, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(model, loader, pred_output_dir):\n",
    "    pred_output_dir = Path(pred_output_dir)\n",
    "    pred_output_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    for batch in tqdm(loader):\n",
    "        img = batch[\"image\"].cuda()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits_mask = model(img)\n",
    "            prob_mask = logits_mask.sigmoid()\n",
    "\n",
    "        # save prob mask as numpy array\n",
    "        for i in range(img.size(0)):\n",
    "            file_name = os.path.basename(batch[\"image_path\"][i])\n",
    "            prob_mask_i = prob_mask[i].cpu().numpy()  # (4, 1024, 1024)\n",
    "\n",
    "            np.save(\n",
    "                pred_output_dir / file_name.replace(\".tif\", \".npy\"),\n",
    "                prob_mask_i.astype(np.float16),\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/tf_efficientnetv2_s.in21k_ft_in1k)\n",
      "INFO:timm.models._hub:[timm/tf_efficientnetv2_s.in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "INFO:timm.models._builder:Converted input conv conv_stem pretrained weights from 3 to 12 channel(s)\n",
      "/tmp/ipykernel_851513/2901745631.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(train_output_dir / \"best_iou_05.ckpt\")[\"state_dict\"])\n",
      "100%|██████████| 9/9 [00:03<00:00,  2.41it/s]\n"
     ]
    }
   ],
   "source": [
    "# load best checkpoint and run inference on val-set\n",
    "\n",
    "del model\n",
    "\n",
    "model = Model()\n",
    "model.load_state_dict(torch.load(train_output_dir / \"best_iou_05.ckpt\")[\"state_dict\"])\n",
    "model = model.cuda()\n",
    "model.eval()\n",
    "\n",
    "val_pred_dir = data_root / \"val_preds\"\n",
    "run_inference(model, val_loader, val_pred_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`detect_polygons()` below extracts isolated areas as polygons from the predicted mask.\n",
    "\n",
    "The point is `min_area` parameter to filter out small polygons. Small polygons are often false positives which largely decrease the evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_polygons(pred_dir, score_thresh, min_area):\n",
    "    pred_dir = Path(pred_dir)\n",
    "    pred_paths = list(pred_dir.glob(\"*.npy\"))\n",
    "    pred_paths = sorted(pred_paths)\n",
    "\n",
    "    polygons_all_imgs = {}\n",
    "    for pred_path in tqdm(pred_paths):\n",
    "        polygons_all_classes = {}\n",
    "\n",
    "        mask = np.load(pred_path)  # (4, 1024, 1024)\n",
    "        for i, class_name in enumerate(class_names):\n",
    "            label = measure.label(mask[i] > score_thresh, connectivity=2, background=0).astype(np.uint8)\n",
    "\n",
    "            polygons = []\n",
    "            for p, value in features.shapes(label, label):\n",
    "                p = shape(p).buffer(0.5)\n",
    "                if p.area >= min_area:\n",
    "                    p = p.simplify(tolerance=0.5)\n",
    "                    polygons.append(p)\n",
    "\n",
    "            polygons_all_classes[class_name] = polygons\n",
    "\n",
    "        polygons_all_imgs[pred_path.name.replace(\".npy\", \".tif\")] = polygons_all_classes\n",
    "\n",
    "    return polygons_all_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:03<00:00, 10.76it/s]\n"
     ]
    }
   ],
   "source": [
    "val_pred_polygons = detect_polygons(val_pred_dir, score_thresh=0.5, min_area=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/176 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 176/176 [00:00<00:00, 1557.78it/s]\n"
     ]
    }
   ],
   "source": [
    "# just load truth polygons from `train_annotations.json` downloaded from the competition page\n",
    "\n",
    "with open(data_root / \"train_annotations.json\", \"r\") as f:\n",
    "    raw_annotations = json.load(f)\n",
    "\n",
    "truth_polygons: dict[str, dict[str, list[Polygon]]] = {}  # file_name -> class_name -> polygons\n",
    "for fn in tqdm([f\"train_{i}.tif\" for i in range(176)]):\n",
    "    ann: dict[str, list[list[float]]] = {}  # class_name -> polygons\n",
    "    for class_name in class_names:\n",
    "        ann[class_name] = []\n",
    "\n",
    "    for tmp_img in raw_annotations[\"images\"]:\n",
    "        if tmp_img[\"file_name\"] == fn:\n",
    "            for tmp_ann in tmp_img[\"annotations\"]:\n",
    "                poly = tmp_ann[\"segmentation\"]\n",
    "                # convert [x1, y1, x2, y2, ..., xn, yn] to [(x1, y1), (x2, y2), ..., (xn, yn)]\n",
    "                new_poly = Polygon([(poly[i], poly[i + 1]) for i in range(0, len(poly), 2)]).buffer(0)\n",
    "                ann[tmp_ann[\"class\"]].append(new_poly)\n",
    "\n",
    "    truth_polygons[fn] = ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average f1 score: 0.08739841623613553\n",
      "\n",
      "f1 scores:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>grassland_shrubland</th>\n",
       "      <th>logging</th>\n",
       "      <th>mining</th>\n",
       "      <th>plantation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train_19.tif</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_45.tif</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_139.tif</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_30.tif</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_67.tif</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_16.tif</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_119.tif</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_172.tif</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_109.tif</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_140.tif</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_24.tif</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_160.tif</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_41.tif</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_118.tif</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_15.tif</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_111.tif</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_113.tif</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_82.tif</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.153846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_9.tif</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_114.tif</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_18.tif</th>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_66.tif</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_60.tif</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_167.tif</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_169.tif</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_150.tif</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_117.tif</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_65.tif</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_90.tif</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_55.tif</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_29.tif</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_127.tif</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_144.tif</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_31.tif</th>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_12.tif</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_42.tif</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.105263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               grassland_shrubland  logging    mining  plantation\n",
       "train_19.tif              0.000000      0.0  0.000000    0.222222\n",
       "train_45.tif              0.000000      0.0  0.000000    0.000000\n",
       "train_139.tif             0.000000      0.0  0.000000    0.222222\n",
       "train_30.tif              0.500000      0.0  0.000000    0.000000\n",
       "train_67.tif              0.000000      0.0  0.142857    0.000000\n",
       "train_16.tif              0.400000      0.0  0.000000    0.000000\n",
       "train_119.tif             0.000000      0.0  0.000000    0.000000\n",
       "train_172.tif             0.000000      0.0  0.000000    0.000000\n",
       "train_109.tif             0.000000      0.0  0.000000    0.285714\n",
       "train_140.tif             0.000000      0.0  0.000000    0.666667\n",
       "train_24.tif              0.000000      0.0  0.000000    0.666667\n",
       "train_160.tif             0.000000      0.0  0.000000    0.000000\n",
       "train_41.tif              0.000000      0.0  0.000000    0.500000\n",
       "train_118.tif             0.000000      0.0  0.000000    0.000000\n",
       "train_15.tif              0.000000      0.0  0.000000    0.400000\n",
       "train_111.tif             0.000000      0.0  0.000000    0.000000\n",
       "train_113.tif             0.000000      0.0  0.000000    0.666667\n",
       "train_82.tif              0.000000      0.0  0.000000    0.153846\n",
       "train_9.tif               1.000000      0.0  0.000000    0.000000\n",
       "train_114.tif             0.000000      0.0  0.000000    0.500000\n",
       "train_18.tif              0.181818      0.0  0.000000    1.000000\n",
       "train_66.tif              0.000000      0.0  0.285714    0.000000\n",
       "train_60.tif              0.000000      0.0  1.000000    0.000000\n",
       "train_167.tif             0.000000      0.0  0.000000    0.000000\n",
       "train_169.tif             0.000000      0.0  0.000000    0.000000\n",
       "train_150.tif             0.400000      0.0  0.000000    0.666667\n",
       "train_117.tif             0.000000      0.0  0.000000    0.400000\n",
       "train_65.tif              0.000000      0.0  0.200000    0.666667\n",
       "train_90.tif              0.000000      0.0  0.000000    0.000000\n",
       "train_55.tif              0.666667      0.0  0.000000    0.000000\n",
       "train_29.tif              0.000000      0.0  0.000000    0.400000\n",
       "train_127.tif             0.000000      0.0  0.000000    0.000000\n",
       "train_144.tif             0.000000      0.0  0.000000    0.000000\n",
       "train_31.tif              0.285714      0.0  0.000000    0.000000\n",
       "train_12.tif              0.000000      0.0  0.000000    0.000000\n",
       "train_42.tif              0.000000      0.0  0.000000    0.105263"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute evaluation metric for validation set\n",
    "\n",
    "metric = F1_Metrics()\n",
    "val_f1_scores = {}\n",
    "\n",
    "for idx in val_indices:\n",
    "    fn = f\"train_{idx}.tif\"\n",
    "    val_f1_scores[fn] = {}\n",
    "    for class_name in class_names:\n",
    "        pred_polys = val_pred_polygons[fn][class_name]\n",
    "        truth_polys = truth_polygons[fn][class_name]\n",
    "        f1_score, _, _ = metric.compute_f1(pred_polys, truth_polys)\n",
    "        val_f1_scores[fn][class_name] = f1_score\n",
    "\n",
    "val_f1_df = pd.DataFrame(val_f1_scores).T\n",
    "\n",
    "val_f1_avg = val_f1_df.mean().mean()  # average of all classes and all images\n",
    "print(f\"average f1 score: {val_f1_avg}\")\n",
    "\n",
    "print()\n",
    "print(\"f1 scores:\")\n",
    "val_f1_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the evaluation images and generate a submission JSON file\n",
    "\n",
    "As already done with the validation set, predict the evaluation images and generate a submission JSON file.\n",
    "\n",
    "The submission JSON file is saved as `data/submission.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_root):\n",
    "        self.image_paths = []\n",
    "        for i in range(118):  # evaluation_0.tif to evaluation_117.tif\n",
    "            self.image_paths.append(data_root / \"evaluation_images\" / f\"evaluation_{i}.tif\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {\n",
    "            \"image\": load_image(self.image_paths[idx]),\n",
    "        }\n",
    "\n",
    "        sample[\"image\"] = sample[\"image\"].transpose(2, 0, 1)  # (12, H, W)\n",
    "        sample[\"image\"] = normalize_image(sample[\"image\"])\n",
    "\n",
    "        # add metadata\n",
    "        sample[\"image_path\"] = str(self.image_paths[idx])\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:09<00:00,  3.32it/s]\n"
     ]
    }
   ],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "    TestDataset(data_root),\n",
    "    batch_size=4,\n",
    "    num_workers=8,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "test_pred_dir = data_root / \"test_preds\"\n",
    "run_inference(model, test_loader, test_pred_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:10<00:00, 10.98it/s]\n"
     ]
    }
   ],
   "source": [
    "test_pred_polygons = detect_polygons(test_pred_dir, score_thresh=0.5, min_area=10000)\n",
    "\n",
    "submission_save_path = data_root / \"submission.json\"\n",
    "\n",
    "images = []\n",
    "for img_id in range(118):  # evaluation_0.tif to evaluation_117.tif\n",
    "    annotations = []\n",
    "    for class_name in class_names:\n",
    "        for poly in test_pred_polygons[f\"evaluation_{img_id}.tif\"][class_name]:\n",
    "            seg: list[float] = []  # [x0, y0, x1, y1, ..., xN, yN]\n",
    "            for xy in poly.exterior.coords:\n",
    "                seg.extend(xy)\n",
    "\n",
    "            annotations.append({\"class\": class_name, \"segmentation\": seg})\n",
    "\n",
    "    images.append({\"file_name\": f\"evaluation_{img_id}.tif\", \"annotations\": annotations})\n",
    "\n",
    "with open(submission_save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"images\": images}, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
